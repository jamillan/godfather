{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "from __future__ import unicode_literals\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jaimemillan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 854,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run in python console\n",
    "import nltk; nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup The enviromental modules\n",
    "\n",
    "### Import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(['english','danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'portuguese', 'russian', 'spanish', 'swedish', 'turkish'])\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get The podcast Data \n",
    "\n",
    "### Here we import the podcast data. At the end we will have about ~100k podcast to utilize for our prediction model. \n",
    "### This csv files offer the following information regarding the podcasts:\n",
    "\n",
    "- podcast description from url and itunes (str)\n",
    "- podcast urls (str)\n",
    "- podcast explicit content (boolean)\n",
    "- podcast name \n",
    "- podcast picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the files Now! \n",
    "### first create string from a- to -z so we can import them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "import string \n",
    "strings = string.ascii_lowercase[1:26]\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df.columns = ['a', 'b','c','d','e','f','g','h','i','j','k','l']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"a.csv\",header=None)\n",
    "#df = pd.read_csv(\"all.csv\")\n",
    "#new_df = pd.read_csv('J_tsar_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col = df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import big files with podcasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], dtype='int64')"
      ]
     },
     "execution_count": 890,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"all.csv\",header=None)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary from numerical colums to podcast descriptions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_num_colnames = {'slug':0,'name':1,'image_url':2,'feed_url':3,'website_url':4,\n",
    "                      'itunes_owner_name':5,'itunes_owner_email':6,\n",
    "                        'description':10,'itunes_summary':11}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_num_colnames = {'slug':0,'name':2,'image_url':3,'feed_url':4,'website_url':5,\n",
    "                      'itunes_owner_name':6,'itunes_owner_email':7,\n",
    "                        'description':11,'itunes_summary':12}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 893,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                   10\n",
       "1    A플러스처치는 독일 다름슈타트에 있는 아름다운교회입니다.&#xD;\\r\\n청년유학생 ...\n",
       "2    A new project dedicated to all lovers of elect...\n",
       "3    We chat about the movies, TV and pop-culturey ...\n",
       "4    <br>Pooopup.com 節目［A基B48 (AGAYB48)]</br>\\n<br>...\n",
       "5    Florence Augusta Merriam Bailey was an America...\n",
       "6    A radio show that plays bluegrass, old time an...\n",
       "7    A blend of soul,pop,funk,rhythm and blues and ...\n",
       "8                                                  NaN\n",
       "9           Another great podcast hosted by LibSyn.com\n",
       "Name: 11, dtype: object"
      ]
     },
     "execution_count": 893,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:10,11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbr_data =1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Clean the podcast\n",
    "\n",
    "### Some podcast description is misssing, or in asian languages hard to deal with. For now we will focus on English.\n",
    "### Suggestion: Extend this analysis to other languages in the future\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backup file to start cleaning process\n",
    "\n",
    "new_df = pd.DataFrame({'content':df.iloc[:,dict_num_colnames['description']],    \n",
    "                       'content2':df.iloc[:,dict_num_colnames['itunes_summary']],\n",
    "                       'name': df.iloc[:,dict_num_colnames['name']],\n",
    "                       'website_url': df.iloc[:,dict_num_colnames['website_url']]\n",
    "                      })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37030\n",
      "92697\n"
     ]
    }
   ],
   "source": [
    "bad_ids= []\n",
    "good_ids = []\n",
    "data = new_df.content[:].values.tolist()\n",
    "data1 = new_df.content2[:].values.tolist()\n",
    "pod_names = new_df.name[:].values.tolist()\n",
    "pod_url = new_df.website_url[:].values.tolist()\n",
    "#data = new_df.text[2:nbr_data].values.tolist()\n",
    "new_data = []\n",
    "for idx,_sentence in enumerate(data):\n",
    "    \n",
    "    if isinstance(_sentence,float):\n",
    "        bad_ids.append(idx)\n",
    "        continue\n",
    "        \n",
    "    tokens = _sentence.split()\n",
    "    new_string = str()\n",
    "    \n",
    "    ADD_STRING = True\n",
    "    for t in tokens:\n",
    "        try:\n",
    "            t.encode('utf-8')\n",
    "            \n",
    "            new_string = new_string +  t.encode('utf-8')+ ' '\n",
    "            \n",
    "        except:\n",
    "            bad_ids.append(idx)\n",
    "            ADD_STRING = False\n",
    "            break\n",
    "    if ADD_STRING:\n",
    "        new_data.append(new_string)\n",
    "        good_ids.append(idx);\n",
    "   \n",
    "print(len(bad_ids))\n",
    "print(len(good_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92697\n",
      "37030\n"
     ]
    }
   ],
   "source": [
    "#bad_ids= []\n",
    "#good_ids = []\n",
    "data = new_df.content[:].values.tolist()\n",
    "data1 = new_df.content2[:].values.tolist()\n",
    "pod_names = new_df.name[:].values.tolist()\n",
    "pod_url = new_df.website_url[:].values.tolist()\n",
    "#data = new_df.text[2:nbr_data].values.tolist()\n",
    "new_data = []\n",
    "\n",
    "def clean_asian(streamdata):\n",
    "    bad_ids= []\n",
    "    good_ids = []\n",
    "    for idx,_sentence in enumerate(streamdata):\n",
    "\n",
    "        if isinstance(_sentence,float):\n",
    "            bad_ids.append(idx)\n",
    "            continue\n",
    "\n",
    "        tokens = _sentence.split()\n",
    "        new_string = str()\n",
    "\n",
    "        ADD_STRING = True\n",
    "        for t in tokens:\n",
    "            try:\n",
    "                t.encode('utf-8')\n",
    "\n",
    "                new_string = new_string +  t.encode('utf-8')+ ' '\n",
    "\n",
    "            except:\n",
    "                bad_ids.append(idx)\n",
    "                ADD_STRING = False\n",
    "                break\n",
    "        if ADD_STRING:\n",
    "            new_data.append(new_string)\n",
    "            good_ids.append(idx);\n",
    "    \n",
    "    return good_ids,bad_ids;\n",
    "\n",
    "good_ids, bad_ids = clean_asian(data)\n",
    "\n",
    "print(len(good_ids))\n",
    "print(len(bad_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92697"
      ]
     },
     "execution_count": 897,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df =new_df.loc[:].drop(bad_ids)\n",
    "data = list(new_data)\n",
    "size =len(final_df);\n",
    "new_index = [i for i in range(size)]\n",
    "final_df.index = new_index\n",
    "len(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92697\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def clean_weird_chars(stream_data):\n",
    "    new_data = []\n",
    "    for idx,sentence in enumerate(stream_data):\n",
    "        new_sentence=str()\n",
    "        try:\n",
    "\n",
    "            new_sentence =  re.sub('\\S*@\\S*\\s?', '', sentence)\n",
    "\n",
    "        except:\n",
    "             continue;\n",
    "\n",
    "        try:\n",
    "\n",
    "            new_sentence =  re.sub('\\s+', ' ', new_sentence)\n",
    "\n",
    "        except:\n",
    "             continue;   \n",
    "\n",
    "        try:\n",
    "\n",
    "            new_sentence =  re.sub(\"\\'\", \"\", new_sentence)\n",
    "\n",
    "        except:\n",
    "             continue;\n",
    "\n",
    "        new_data.append(new_sentence)\n",
    "\n",
    "    return new_data;\n",
    "new_data = clean_weird_chars(data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_df.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 900,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92697\n",
      "92697\n"
     ]
    }
   ],
   "source": [
    "# Convert to list\n",
    "\n",
    "#data = new_df.content[:50].values.tolist()\n",
    "\n",
    "\n",
    "# Remove Emails\n",
    "new_data = []\n",
    "for idx,sentence in enumerate(data):\n",
    "    new_sentence=str()\n",
    "    try:\n",
    "        \n",
    "        new_sentence =  re.sub('\\S*@\\S*\\s?', '', sentence)\n",
    "        \n",
    "    except:\n",
    "         continue;\n",
    "            \n",
    "    try:\n",
    "        \n",
    "        new_sentence =  re.sub('\\s+', ' ', new_sentence)\n",
    "        \n",
    "    except:\n",
    "         continue;   \n",
    "    \n",
    "    try:\n",
    "        \n",
    "        new_sentence =  re.sub(\"\\'\", \"\", new_sentence)\n",
    "        \n",
    "    except:\n",
    "         continue;\n",
    "\n",
    "    new_data.append(new_sentence)\n",
    "    \n",
    "     \n",
    "# Remove email\n",
    "#new_sentence =  re.sub('\\S*@\\S*\\s?', '', sentence)\n",
    "\n",
    "# Remove new line characters\n",
    "#new_sentence =  re.sub('\\s+', ' ', new_sentence)\n",
    "\n",
    "# Remove distracting single quotes\n",
    "#new_sentence =  re.sub(\"\\'\", \"\", new_sentence)\n",
    "\n",
    "#pprint(new_data[:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92697"
      ]
     },
     "execution_count": 870,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = new_data\n",
    "final_df.content = data\n",
    "len(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'10 ',\n",
       " u'A new project dedicated to all lovers of electronic music ',\n",
       " u'We chat about the movies, TV and pop-culturey stuff ',\n",
       " u'Florence Augusta Merriam Bailey was an American ornithologist and nature writer. She started observing bird behavior at a time when most bird study was based on collections and skins. By 1885, she began to write articles focusing on protecting birds. Her introduction of a birdwatching field guide, aimed at living birds, is considered the first in the tradition of modern bird guides. She wrote the first of these at the age of 26, initially as a series of notes in the Audubon Magazine and later as books. In \"A-Birding on a Bronco,\" she writes an engaging memoir about her several trips to study birds on a ranch in California in the late 1800s. ',\n",
       " u'A radio show that plays bluegrass, old time and gospel music only by unsigned bands and discusses their playing style. ',\n",
       " u'A blend of soul,pop,funk,rhythm and blues and rock : songs by Musilosophy, singer songwriter and jazz pianist.&lt;br&gt; ',\n",
       " u'Another great podcast hosted by LibSyn.com ',\n",
       " u'A semi-weekly discussion of the plots, themes and highlights of the HBO series \"Game of Thrones.\" Your hosts Dave Chen (The /Filmcast and The Tobolowsky Files) and Joanna Robinson (an Editor for Pajiba.com) will dish up both praise and criticism and will try not to sound too pompous if the book was better. Questions, comments, oaths of fealty? Email ',\n",
       " u'Ebenezer Scrooge, a mean old miser, is given a second chance to do right after being haunted by three ghosts on Christmas Eve in this Dickens classic. The Christmas Carol Audio Book. ',\n",
       " u'ideas for life ']"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'new', u'project', u'dedicated', u'to', u'all', u'lovers', u'of', u'electronic', u'music'], [u'we', u'chat', u'about', u'the', u'movies', u'tv', u'and', u'pop', u'culturey', u'stuff']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        \n",
    "        try:\n",
    "            a = gensim.utils.simple_preprocess(str(sentence), deacc=True)\n",
    "            \n",
    "            yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "        except:\n",
    "            \n",
    "            continue\n",
    "        \n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:2])\n",
    "del data_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaimemillan/miniconda2/lib/python2.7/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'florence', u'augusta', u'merriam', u'bailey', u'was', u'an', u'american', u'ornithologist', u'and', u'nature', u'writer', u'she', u'started', u'observing', u'bird', u'behavior', u'at', u'time', u'when', u'most', u'bird', u'study', u'was', u'based', u'on', u'collections', u'and', u'skins', u'by', u'she', u'began', u'to', u'write', u'articles', u'focusing', u'on', u'protecting', u'birds', u'her', u'introduction', u'of', u'birdwatching', u'field', u'guide', u'aimed', u'at', u'living', u'birds', u'is', u'considered', u'the', u'first', u'in', u'the', u'tradition', u'of', u'modern', u'bird', u'guides', u'she', u'wrote', u'the', u'first', u'of', u'these', u'at', u'the', u'age', u'of', u'initially', u'as', u'series', u'of', u'notes', u'in', u'the', u'audubon', u'magazine', u'and', u'later', u'as', u'books', u'in', u'birding', u'on', u'bronco', u'she', u'writes', u'an', u'engaging', u'memoir', u'about', u'her', u'several', u'trips', u'to', u'study', u'birds', u'on', u'ranch', u'in', u'california', u'in', u'the', u'late']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'florence', u'augusta', u'merriam', u'bailey', u'american', u'ornithologist', u'nature', u'writer', u'start', u'observe', u'bird', u'behavior', u'time', u'bird', u'study', u'base', u'collection', u'skin', u'begin', u'write', u'article', u'focus', u'protect', u'bird', u'introduction', u'birdwatching', u'field', u'guide', u'aim', u'live', u'bird', u'consider', u'first', u'tradition', u'modern', u'bird', u'guide', u'write', u'first', u'age', u'initially', u'series', u'note', u'magazine', u'later', u'book', u'bird', u'bronco', u'write', u'engage', u'memoir', u'several', u'trip', u'study', u'bird', u'ranch', u'california', u'late'], [u'radio', u'show', u'play', u'bluegrass', u'old', u'time', u'gospel', u'music', u'unsigned', u'band', u'discuss', u'play', u'style'], [u'blend', u'soul', u'pop', u'funk', u'rhythm', u'blue', u'rock', u'song', u'musilosophy', u'singer', u'songwriter', u'jazz', u'pianist', u'lt_br', u'gt'], [u'great', u'podcast', u'host', u'libsyn'], [u'semi', u'weekly', u'discussion', u'plot', u'theme', u'highlight', u'hbo', u'series', u'game', u'throne', u'host', u'dave', u'chen', u'filmcast', u'tobolowsky', u'file', u'joanna', u'robinson', u'editor', u'pajiba', u'dish', u'praise', u'criticism', u'try', u'sound', u'pompous', u'book', u'good', u'question', u'comment', u'oath', u'fealty', u'email'], [u'ebenezer', u'scrooge', u'mean', u'old', u'miser', u'give', u'second', u'chance', u'right', u'haunt', u'ghost', u'christma', u'eve', u'dicken', u'classic', u'christma', u'carol', u'audio', u'book'], [u'idea', u'life'], [u'idea', u'life'], [u'center', u'strong', u'nothing', u'real', u'threaten', u'lt_br', u'nothing', u'unreal', u'exist', u'herein', u'lie', u'peace', u'strong', u'gt', u'center'], [u'quot', u'cuppa', u'miracle', u'quot', u'series', u'cuppa', u'miracle', u'back', u'send', u'idea', u'topic', u'would', u'like', u'show', u'join', u'minute', u'series', u'share', u'personal', u'experience', u'wisdom', u'course', u'miracl', u'lt_br', u'gt', u'show', u'create', u'moment', u'simply', u'ask', u'holy', u'spirit', u'topic', u'share', u'cup', u'coffee', u'set', u'camera', u'roll', u'lt_br', u'grab', u'cuppa', u'coffee', u'tea', u'back', u'enjoy', u'quot', u'cuppa', u'miracle', u'quot', u'note', u'audio', u'youtube', u'series', u'name', u'miraclesone', u'foundation', u'www', u'miraclesone', u'org']]\n"
     ]
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=2, \n",
    "                                           random_state=400,\n",
    "                                           update_every=100,\n",
    "                                           chunksize=10,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TOPICS = 10\n",
    "lsi_model = gensim.models.LsiModel(corpus=corpus, num_topics=N_TOPICS, id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "#print('\\nPerplexity: ', lsi_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lsi = CoherenceModel(model=lsi_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lsi = coherence_model_lsi.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LSI Model:\")\n",
    " \n",
    "for idx in range(N_TOPICS):\n",
    "    # Print the first 10 most representative topics\n",
    "    print(\"Topic #%s:\" % idx, lsi_model.print_topic(idx, 10))\n",
    " \n",
    "print(\"=\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TOPICS = 5\n",
    "lda_model = gensim.models.LdaModel(corpus=corpus, num_topics=N_TOPICS, id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LDA Model:\")\n",
    " \n",
    "for idx in range(N_TOPICS):\n",
    "    # Print the first 10 most representative topics\n",
    "    print(\"Topic #%s:\" % idx, lda_model.print_topic(idx, 10))\n",
    " \n",
    "print(\"=\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('J_tsar_tweets.csv')\n",
    "test_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a9eea11dc96f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "text = test_data.text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-4ba346c9fb70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Remove Emails\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mnew_sentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "# Convert to list\n",
    "\n",
    "#data = new_df.content[:50].values.tolist()\n",
    "\n",
    "\n",
    "# Remove Emails\n",
    "new_data = []\n",
    "for idx,sentence in enumerate(text):\n",
    "    new_sentence=str()\n",
    "    try:\n",
    "        \n",
    "        new_sentence =  re.sub('\\S*@\\S*\\s?', '', sentence)\n",
    "        \n",
    "    except:\n",
    "         continue;\n",
    "            \n",
    "    try:\n",
    "        \n",
    "        new_sentence =  re.sub('\\s+', ' ', new_sentence)\n",
    "        \n",
    "    except:\n",
    "         continue;   \n",
    "    \n",
    "    try:\n",
    "        \n",
    "        new_sentence =  re.sub(\"\\'\", \"\", new_sentence)\n",
    "        \n",
    "    except:\n",
    "         continue;\n",
    "\n",
    "    new_data.append(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_text = list()\n",
    "for tweet in new_data[:20]:\n",
    "    tokens = tweet.split()\n",
    "    \n",
    "    string_text += tokens\n",
    "    \n",
    "#print(string_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'id2word' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-498e9ed627e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'id2word' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "bow = id2word.doc2bow(string_text)\n",
    " \n",
    "print(lda_model[bow])\n",
    "# [(0, 0.091615426138426506), (1, -0.0085557463300508351), (2, 0.016744863677828108), (3, 0.040508186718598529), (4, 0.014201267714185898), (5, -0.012208538275305329), (6, 0.031254053085582149), (7, 0.017529584659403553), (8, 0.056957633371540077),\n",
    "#(9, 0.025989149894888153)]\n",
    " \n",
    "#print(lda_model[bow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lsi_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-0002271bf301>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlsi_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lsi_model' is not defined"
     ]
    }
   ],
   "source": [
    "print(lsi_model[bow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lda_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-45b87b707f69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtop_words_per_topic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtop_words_per_topic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlda_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_topic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_words_per_topic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Topic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Word'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'P'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"top_words.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lda_model' is not defined"
     ]
    }
   ],
   "source": [
    "top_words_per_topic = []\n",
    "for t in range(lda_model.num_topics):\n",
    "    top_words_per_topic.extend([(t, ) + x for x in lda_model.show_topic(t, topn = 20)])\n",
    "\n",
    "pd.DataFrame(top_words_per_topic, columns=['Topic', 'Word', 'P']).to_csv(\"top_words.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_lda = pd.read_csv('top_words.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Word</th>\n",
       "      <th>P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>right</td>\n",
       "      <td>0.024787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>series</td>\n",
       "      <td>0.022096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>show</td>\n",
       "      <td>0.020418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>quot</td>\n",
       "      <td>0.015878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>episode</td>\n",
       "      <td>0.013829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>hard</td>\n",
       "      <td>0.012986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>christma</td>\n",
       "      <td>0.012983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>keep</td>\n",
       "      <td>0.011998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>audio</td>\n",
       "      <td>0.011799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>get</td>\n",
       "      <td>0.011732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>br</td>\n",
       "      <td>0.009586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>book</td>\n",
       "      <td>0.008316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>follow</td>\n",
       "      <td>0.007318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>world</td>\n",
       "      <td>0.007299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>history</td>\n",
       "      <td>0.007240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>podcast</td>\n",
       "      <td>0.007106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>old</td>\n",
       "      <td>0.007087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>time</td>\n",
       "      <td>0.007086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>eve</td>\n",
       "      <td>0.007086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>good</td>\n",
       "      <td>0.007085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>br</td>\n",
       "      <td>0.019376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>idea</td>\n",
       "      <td>0.013865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>battle</td>\n",
       "      <td>0.012151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>place</td>\n",
       "      <td>0.012148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>life</td>\n",
       "      <td>0.012040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>amp</td>\n",
       "      <td>0.010019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>nbsp</td>\n",
       "      <td>0.009661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>0.009017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>quot</td>\n",
       "      <td>0.008975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>cuppa</td>\n",
       "      <td>0.008925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>listen</td>\n",
       "      <td>0.007653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>71</td>\n",
       "      <td>3</td>\n",
       "      <td>play</td>\n",
       "      <td>0.007650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>72</td>\n",
       "      <td>3</td>\n",
       "      <td>fine</td>\n",
       "      <td>0.007647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>73</td>\n",
       "      <td>3</td>\n",
       "      <td>groove</td>\n",
       "      <td>0.007646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>74</td>\n",
       "      <td>3</td>\n",
       "      <td>hope</td>\n",
       "      <td>0.007646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>75</td>\n",
       "      <td>3</td>\n",
       "      <td>house</td>\n",
       "      <td>0.007646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>76</td>\n",
       "      <td>3</td>\n",
       "      <td>musical</td>\n",
       "      <td>0.007646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>77</td>\n",
       "      <td>3</td>\n",
       "      <td>stroll</td>\n",
       "      <td>0.007646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>78</td>\n",
       "      <td>3</td>\n",
       "      <td>london</td>\n",
       "      <td>0.007646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>79</td>\n",
       "      <td>3</td>\n",
       "      <td>sunday</td>\n",
       "      <td>0.007645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>80</td>\n",
       "      <td>4</td>\n",
       "      <td>br</td>\n",
       "      <td>0.038597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>81</td>\n",
       "      <td>4</td>\n",
       "      <td>gt</td>\n",
       "      <td>0.015491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>82</td>\n",
       "      <td>4</td>\n",
       "      <td>center</td>\n",
       "      <td>0.015097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>83</td>\n",
       "      <td>4</td>\n",
       "      <td>play</td>\n",
       "      <td>0.015096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>84</td>\n",
       "      <td>4</td>\n",
       "      <td>strong</td>\n",
       "      <td>0.015096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>85</td>\n",
       "      <td>4</td>\n",
       "      <td>nothing</td>\n",
       "      <td>0.015085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>86</td>\n",
       "      <td>4</td>\n",
       "      <td>back</td>\n",
       "      <td>0.013021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>87</td>\n",
       "      <td>4</td>\n",
       "      <td>show</td>\n",
       "      <td>0.012378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>88</td>\n",
       "      <td>4</td>\n",
       "      <td>miracle</td>\n",
       "      <td>0.012349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>89</td>\n",
       "      <td>4</td>\n",
       "      <td>quot</td>\n",
       "      <td>0.011436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>90</td>\n",
       "      <td>4</td>\n",
       "      <td>cuppa</td>\n",
       "      <td>0.010687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>91</td>\n",
       "      <td>4</td>\n",
       "      <td>series</td>\n",
       "      <td>0.008860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>92</td>\n",
       "      <td>4</td>\n",
       "      <td>life</td>\n",
       "      <td>0.008264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>93</td>\n",
       "      <td>4</td>\n",
       "      <td>great</td>\n",
       "      <td>0.008250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>94</td>\n",
       "      <td>4</td>\n",
       "      <td>radio</td>\n",
       "      <td>0.008237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>4</td>\n",
       "      <td>herein</td>\n",
       "      <td>0.008236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>4</td>\n",
       "      <td>old</td>\n",
       "      <td>0.008235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>4</td>\n",
       "      <td>bluegrass</td>\n",
       "      <td>0.008235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>4</td>\n",
       "      <td>conquest</td>\n",
       "      <td>0.008235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>4</td>\n",
       "      <td>lie</td>\n",
       "      <td>0.008235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  Topic       Word         P\n",
       "0            0      0      right  0.024787\n",
       "1            1      0     series  0.022096\n",
       "2            2      0       show  0.020418\n",
       "3            3      0       quot  0.015878\n",
       "4            4      0    episode  0.013829\n",
       "5            5      0       hard  0.012986\n",
       "6            6      0   christma  0.012983\n",
       "7            7      0       keep  0.011998\n",
       "8            8      0      audio  0.011799\n",
       "9            9      0        get  0.011732\n",
       "10          10      0         br  0.009586\n",
       "11          11      0       book  0.008316\n",
       "12          12      0     follow  0.007318\n",
       "13          13      0      world  0.007299\n",
       "14          14      0    history  0.007240\n",
       "15          15      0    podcast  0.007106\n",
       "16          16      0        old  0.007087\n",
       "17          17      0       time  0.007086\n",
       "18          18      0        eve  0.007086\n",
       "19          19      0       good  0.007085\n",
       "20          20      1         br  0.019376\n",
       "21          21      1       idea  0.013865\n",
       "22          22      1     battle  0.012151\n",
       "23          23      1      place  0.012148\n",
       "24          24      1       life  0.012040\n",
       "25          25      1        amp  0.010019\n",
       "26          26      1       nbsp  0.009661\n",
       "27          27      1     series  0.009017\n",
       "28          28      1       quot  0.008975\n",
       "29          29      1      cuppa  0.008925\n",
       "..         ...    ...        ...       ...\n",
       "70          70      3     listen  0.007653\n",
       "71          71      3       play  0.007650\n",
       "72          72      3       fine  0.007647\n",
       "73          73      3     groove  0.007646\n",
       "74          74      3       hope  0.007646\n",
       "75          75      3      house  0.007646\n",
       "76          76      3    musical  0.007646\n",
       "77          77      3     stroll  0.007646\n",
       "78          78      3     london  0.007646\n",
       "79          79      3     sunday  0.007645\n",
       "80          80      4         br  0.038597\n",
       "81          81      4         gt  0.015491\n",
       "82          82      4     center  0.015097\n",
       "83          83      4       play  0.015096\n",
       "84          84      4     strong  0.015096\n",
       "85          85      4    nothing  0.015085\n",
       "86          86      4       back  0.013021\n",
       "87          87      4       show  0.012378\n",
       "88          88      4    miracle  0.012349\n",
       "89          89      4       quot  0.011436\n",
       "90          90      4      cuppa  0.010687\n",
       "91          91      4     series  0.008860\n",
       "92          92      4       life  0.008264\n",
       "93          93      4      great  0.008250\n",
       "94          94      4      radio  0.008237\n",
       "95          95      4     herein  0.008236\n",
       "96          96      4        old  0.008235\n",
       "97          97      4  bluegrass  0.008235\n",
       "98          98      4   conquest  0.008235\n",
       "99          99      4        lie  0.008235\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'9 ', u'False ', u'False ']"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 879,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "#data = [u'hello I love you more hi bye',u'hello I love you more hi bye say ', u'hello I love you more hi bye class class',u'create a python sql cpp',u'contact your mom dad son']\n",
    "y = [int(1) for i in range(len(data))]\n",
    "\n",
    "#X_test = [u'The people of Germany are turning against their leadershipas migration is rocking the already tenuous Berlin coalition. Crime in Germany is way up. Big mistake made all over Europe in allowing millions of people in who have so strongly and violently changed their culture!t!']\n",
    "#X_test = [u'fussball fussball']\n",
    "#X_test = ['Hey Joe,Wondering why this website: http://www.iiseagrant.org/wilmettebuoy/  and this website: http://www.ndbc.noaa.gov/station_page.php?station=45174 … show different wind dir / speed']\n",
    "X_test =['Low plastic stool, cheap but delicious noodles, cold Hanoi beer.” This is how I’ll remember Tony. He taught us about food — but more importantly, about its ability to bring us together. To make us a little less afraid of the unknown. We’ll miss him.']\n",
    "# Transform the training data: tfidf_train \n",
    "#X_test = ['I live in a border state. border state I appreciate the need to enforce and protect our international boundaries, but this zero-tolerance policy is cruel. It is immoral. And it breaks my heart.']\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(data)\n",
    "\n",
    "# Transform the test data: tfidf_test \n",
    "tfidf_test = tfidf_vectorizer.transform(X_test)\n",
    "# Initialize a TfidfVectorizer object: tfidf_vectorizer\n",
    "\n",
    "\n",
    "modelknn = KNeighborsClassifier(n_neighbors=1)\n",
    "modelknn.fit(tfidf_train,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]]\n",
      "[[    0 60481 60480 ... 34970 86125 65352]]\n"
     ]
    }
   ],
   "source": [
    "a= np.dot(tfidf_test,tfidf_train.T)\n",
    "print(a.A)\n",
    "a_sort = np.argsort(a.A)\n",
    "print(a_sort)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 50)"
      ]
     },
     "execution_count": 881,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(a.A.flatten()[:])\n",
    "plt.xlim((0.2,0.35))\n",
    "plt.ylim((0,50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vals_idx =  a_sort.flatten()[::-1][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([65352, 86125, 34970, 31470, 78878, 31436, 87831, 44817, 24360,\n",
       "       69511, 72278, 20317, 46116, 52301, 70935, 81162, 50469, 64048,\n",
       "       75528, 87650,  3763, 10912, 59668, 73096, 50926, 45979, 34981,\n",
       "       56869, 55979, 59383,  1890,  9039, 37095, 16787, 53223, 78392,\n",
       "       74451, 91082, 15891, 79597, 63904, 19745, 89374, 67129, 90407,\n",
       "       62167, 15892,  1329,  8478, 15480,   413,  7562,   580,  7729,\n",
       "       75968, 27493, 34638, 16245, 16244, 80081, 82240, 15880, 15896,\n",
       "       42432, 15907, 50638, 81439, 35904, 33968, 27655, 40964, 26531,\n",
       "       35497, 44726, 19212, 61854, 15054, 26524, 12335,  5186, 17331,\n",
       "       29353, 15900, 63665, 15914, 15913, 51873, 44595, 10075,  2926,\n",
       "       77099, 38852,  8790,  1641, 26534, 37457, 87576, 15915, 33984,\n",
       "       69517])"
      ]
     },
     "execution_count": 883,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_vals_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = final_df.loc[max_vals_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65352                http://www.supraliminalfilms.com\n",
       "86125               http://djscenx.universpodcast.com\n",
       "34970                 http://craftbeernation.org/blog\n",
       "31470     http://www.blogtalkradio.com/everydaycookin\n",
       "78878                  http://superbudgetbrothers.com\n",
       "31436                        http://blog.evernote.com\n",
       "87831          http://visibledrugs.libsyn.com/webpage\n",
       "44817                        http://jandapairings.com\n",
       "24360                    http://chickndickpodcast.com\n",
       "69511                           http://www.ivoox.com/\n",
       "72278     http://www.savethecat.com/category/podcasts\n",
       "20317             http://www.thebryancrabtreeshow.com\n",
       "46116                     http://www.beerfellows.com/\n",
       "52301         https://www.facebook.com/deejayerikt.fr\n",
       "70935                          http://rogueintel.com/\n",
       "81162    http://feeds.feedburner.com/CTR-ThatFunnyGal\n",
       "50469                        http://www.beerloons.com\n",
       "64048                           http://please-bite.me\n",
       "75528                   http://sobeadam.wordpress.com\n",
       "87650                        http://www.vikandray.com\n",
       "Name: website_url, dtype: object"
      ]
     },
     "execution_count": 885,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.website_url[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Teaching you to make films on the cheap...or we&#039;ll die trying. ',\n",
       " u'Together, we&#039;ll make the difference ! ',\n",
       " u'Cold beer and good friends. ',\n",
       " u'Its all about food! Remember, food is my life and life is my food! ',\n",
       " u'Cheap games for cheap gamers ',\n",
       " u'Remember Everything. ',\n",
       " u'Soft Dudes. Cheap beer. ',\n",
       " u'Wine. Beer. Food. ',\n",
       " u'A free comedy podcast for adults. We&#039;ll make you a better person, or your money back. ',\n",
       " u'Podcast Remember Me! ',\n",
       " u'The Last Website on Screenwriting You&#039;ll Ever Need ',\n",
       " u'The longer you listen, the smarter you&#039;ll get ',\n",
       " u'Its all about beer ',\n",
       " u'French DJ, Deejay Erik T ll Resident Le Guss (Cholet, FR) Every Week ll #MSNS. ',\n",
       " u'The most interesting degenerates you&#039;ll ever hear. ',\n",
       " u'Join me, Kiera Giordan, every week as I interview comics and other people in the business of being funny. We&apos;ll be talking about the journey of the comics. The highs and the lows. We&apos;ll get personal about what a comic&apos;s life is really like. We&apos;ll talk about what makes us laugh. The art of being funny. Tips and tricks of the trade. We&apos;ll also spotlight upcoming shows and events that make us excited. Plus, it wouldn&apos;t be a show about comedy without a lot of jokes being thrown around as well. Join me for a very funny hour that will also expand your mind about the business of making people laugh.&lt;br /&gt; ',\n",
       " u'A weekly how to program on how to make beer. ',\n",
       " u'I&#039;m not sure yet but when I find out I&#039;ll let you know ',\n",
       " u'Not The Worst You&#039;ll Ever See, And That&#039;s Our Guarantee ',\n",
       " u'the sexiest voices you&#039;ll ever wake up to ',\n",
       " u'A show for people that are not afraid to laugh. ',\n",
       " u'A show for people that are not afraid to laugh. ',\n",
       " u'A radio broadcasting from unknown place for unknown reason ',\n",
       " u'Together we&#039;ll create a sane and rational world! ',\n",
       " u'LL Cool J is an entertainment icon who has found great success crossing into multiple mediums. A two-time Grammy Award winner, recording artist, talented actor, author, NAACP Image Award winner, entrepreneur, and philanthropist, LL continues to display his wide range of talents with every project. Celebrating three decades in the music business, LL just released his 13th Billboard top-10 R&amp;B/Hip-Hop album, \"Authentic.\" ',\n",
       " u'I write lots of songs--hopefully there&amp;#39;ll be a few that you like!!! ',\n",
       " u'Craft Beer experts discuss craft beer, brewing, beer news and other beer related topics, but mostly beer. | | | | ',\n",
       " u'Cheap laughs for free! ',\n",
       " u'Every other Monday we make your Disney memories com to life! Make sure and subscribe to never miss another exciting episode! Thanks for checking us out and remember Adventure is out there! ',\n",
       " u'This is the Nosheteria, kind of like a cafeteria, where bites on food are served up to you like little morsels of goodness. Here you&apos;ll get a little bit of everything, the occasional recipe, the amusing observation, and the always entertaining story about my life (you&apos;re expected to care) in food. Haute cuisine for the masses! ',\n",
       " u'An Australian podcast about beer and beer stuff. ',\n",
       " u'An Australian podcast about beer and beer stuff. ',\n",
       " u'Get Low: Get Low is now available. Get Low Featurette: Get Low Featurette: Behind the Scenes ',\n",
       " u'Saving Local Agencies From Bankruptcy Using the Three-Legged Stool System of Pavement Management ',\n",
       " u'The Matt and Tony Show ',\n",
       " u'Studio 17141 Three average Joes from Flint, Michigan, who shoot the breeze on everything political and cultural in our state. They play no favorites, make no assumptions, and are equal opportunity critics of anything, with some awesome bumper music! Check out our videos for some of the best commentary on Michigan you&apos;ll hear and see and you&apos;ll even get a laugh too. Make Your Voice Heard! Call In! This Podcast was created using www.talkshoe.com ',\n",
       " u'Beer ,Women, Beer, Fun, Beer, Sillyness, Beer, and sometimes intelligence. Did we mention Beer? Ok these guys have no intelligence, but they sure are fun to listen to ramble on about life in NYC. And Beer. ',\n",
       " u'The vidcast of the wowvidsuk youtube channel bring you their new wow podcast. Each week we&apos;ll bring you the latest WoW news ',\n",
       " u'Beer Hear! is a weekly program on beer, beer culture and the beer community, heard on WFMU (wfmu.org). There is an accompanying blog at: beerhear.blogspot.com. Beer Hear! is hosted by Bob & B.R., homebrewers, beer judges, and all around beer lovers. ',\n",
       " u'Tales From the Cask is a weekly beer podcast dedicated to craft beer enthusiasts. We all work in the beer industry and are passionate about beer. We share knowledge and news about beer, interview special guests within the industry, sample beers from around the country, and tell fun stories of our experiences with beer. ',\n",
       " u'Plastic is the official Academy of Barcelona for club DJs since 1993. In 20 years, handled by an experienced group of DJs and professional producers, PLASTIC has trained more than 2.000 students. This radio show demonstrates how new talent, using Pioneer DJ Technology , supervised by Plastic&#8217;s coaches, can become real stars of the whole world&#8217;s night clubs. Amazing sessions of outstanding students and teachers of Plastic, the official Club Dj Academy of Barcelona. ',\n",
       " u'Craft Beer reviews for beer lovers. ',\n",
       " u'Now that&#039;s a beer! ',\n",
       " u'Thank you all for your support and kindness. We have relocated to www.nowlive.com/quietontheset. I hope you&apos;ll stop by, register and add us as friends. And more importantly, join us during our show! This Podcast was created using www.talkshoe.com ',\n",
       " u'Listen in every week as the Wired product team waxes poetic about the gadget news of the past few days. It may not change your life, but you&#39;ll hear our take on a range of products, from the latest laptop computers to the hottest geek toys on the planet. Laugh. Cry. Learn. Love. Just tune in. We&#39;ll do the rest. ',\n",
       " u'talk show about food ',\n",
       " u'Beer review show by two guys who love Beer, in HD. ',\n",
       " u'Named after my blog site, http://afterthegames.blogspot.com, I&apos;ll be discussing just about anything that comes up. I&apos;ll also from time to time discuss social issues or Biblical issues. If you want to call in with an idea, feel free to do so. Mondays and Wednesdays 9:30PM-10:30PM This Podcast was created using www.talkshoe.com ',\n",
       " u'Named after my blog site, http://afterthegames.blogspot.com, I&apos;ll be discussing just about anything that comes up. I&apos;ll also from time to time discuss social issues or Biblical issues. If you want to call in with an idea, feel free to do so. Mondays and Wednesdays 9:30PM-10:30PM This Podcast was created using www.talkshoe.com ',\n",
       " u'&#8220;Be Indie Now!&#8221; is a podcast about independent game development. Join Tobiah Marks as your host and moderator as each episode he will talk to guest panelists on a wide range of game development topics. From the design, to the technical, to the business and marketing, we&#8217;ll cover everything you&#8217;ll need to know to create your own games. ']"
      ]
     },
     "execution_count": 886,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podcast_descriptions = []\n",
    "for idx in max_vals_idx:\n",
    "    podcast_descriptions.append(data[idx]) \n",
    "\n",
    "podcast_descriptions[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'10 '"
      ]
     },
     "execution_count": 809,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 810,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "modelknn.predict(tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 811,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('J_tsar_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweet = test_data.text.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ain't no love in the heart of the city, stay safe people\""
      ]
     },
     "execution_count": 712,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tweet[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    test_str =  test_str + test_tweet[i] + \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'@MelloChamp and they what \"god hates dead people?\" Or victims of tragedies? Lol those people are cooked RT @muftimenk: Attitude can take away your beauty no matter how good looking you are or it could enhance your beauty, making you adorable. I\\'m a stress free kind of guy Nowadays everybody wanna talk like they got somethin to say but nothin comes out when they move their lips; just a bunch of gibberish @ImRealTed fake story So then I says to him, I says, relax bro my beard is not loaded @therealAbdul_ #heavy I\\'ve been looking for those, there is a shortage on the black market if you wanna make a quick buck, nuff said.. @therealAbdul_ you need to get Claritin clear There are people that know the truth but stay silent &amp; there are people that speak the truth but we don\\'t hear them cuz they\\'re the minority @mellochamp *say @MelloChamp and they what \"god hates dead people?\" Or victims of tragedies? Lol those people are cooked RT @muftimenk: Attitude can take away your beauty no matter how good looking you are or it could enhance your beauty, making you adorable. I\\'m a stress free kind of guy Nowadays everybody wanna talk like they got somethin to say but nothin comes out when they move their lips; just a bunch of gibberish @ImRealTed fake story So then I says to him, I says, relax bro my beard is not loaded @therealAbdul_ #heavy I\\'ve been looking for those, there is a shortage on the black market if you wanna make a quick buck, nuff said.. @therealAbdul_ you need to get Claritin clear There are people that know the truth but stay silent &amp; there are people that speak the truth but we don\\'t hear them cuz they\\'re the minority @mellochamp *say @MelloChamp and they what \"god hates dead people?\" Or victims of tragedies? Lol those people are cooked '"
      ]
     },
     "execution_count": 714,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
