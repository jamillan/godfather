{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "from __future__ import unicode_literals\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jaimemillan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run in python console\n",
    "import nltk; nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(['english','danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'portuguese', 'russian', 'spanish', 'swedish', 'turkish'])\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "import string \n",
    "strings = string.ascii_lowercase[1:26]\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df.columns = ['a', 'b','c','d','e','f','g','h','i','j','k','l']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"a.csv\",header=None)\n",
    "#new_df = pd.read_csv('J_tsar_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], dtype='int64')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbr_data =200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame({'content':df.iloc[:,10]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_ids= []\n",
    "data = new_df.content[2:nbr_data].values.tolist()\n",
    "#data = new_df.text[2:nbr_data].values.tolist()\n",
    "new_data = []\n",
    "for idx,_sentence in enumerate(data):\n",
    "    \n",
    "    if isinstance(_sentence,float):\n",
    "        bad_ids.append(idx)\n",
    "        continue\n",
    "        \n",
    "    tokens = _sentence.split()\n",
    "    new_string = str()\n",
    "    \n",
    "    ADD_STRING = True\n",
    "    for t in tokens:\n",
    "        try:\n",
    "            t.encode('utf-8')\n",
    "            \n",
    "            new_string = new_string +  t.encode('utf-8')+ ' ' \n",
    "        except:\n",
    "            bad_ids.append(idx)\n",
    "            ADD_STRING = False\n",
    "            break\n",
    "    if ADD_STRING:\n",
    "        new_data.append(new_string)\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = list(new_data)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "\n",
    "#data = new_df.content[:50].values.tolist()\n",
    "\n",
    "\n",
    "# Remove Emails\n",
    "new_data = []\n",
    "for idx,sentence in enumerate(data):\n",
    "    new_sentence=str()\n",
    "    try:\n",
    "        \n",
    "        new_sentence =  re.sub('\\S*@\\S*\\s?', '', sentence)\n",
    "        \n",
    "    except:\n",
    "         continue;\n",
    "            \n",
    "    try:\n",
    "        \n",
    "        new_sentence =  re.sub('\\s+', ' ', new_sentence)\n",
    "        \n",
    "    except:\n",
    "         continue;   \n",
    "    \n",
    "    try:\n",
    "        \n",
    "        new_sentence =  re.sub(\"\\'\", \"\", new_sentence)\n",
    "        \n",
    "    except:\n",
    "         continue;\n",
    "\n",
    "    new_data.append(new_sentence)\n",
    "        \n",
    "# Remove new line characters\n",
    "#data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "#data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "#pprint(new_data[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = new_data\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'we', u'chat', u'about', u'the', u'movies', u'tv', u'and', u'pop', u'culturey', u'stuff'], [u'florence', u'augusta', u'merriam', u'bailey', u'was', u'an', u'american', u'ornithologist', u'and', u'nature', u'writer', u'she', u'started', u'observing', u'bird', u'behavior', u'at', u'time', u'when', u'most', u'bird', u'study', u'was', u'based', u'on', u'collections', u'and', u'skins', u'by', u'she', u'began', u'to', u'write', u'articles', u'focusing', u'on', u'protecting', u'birds', u'her', u'introduction', u'of', u'birdwatching', u'field', u'guide', u'aimed', u'at', u'living', u'birds', u'is', u'considered', u'the', u'first', u'in', u'the', u'tradition', u'of', u'modern', u'bird', u'guides', u'she', u'wrote', u'the', u'first', u'of', u'these', u'at', u'the', u'age', u'of', u'initially', u'as', u'series', u'of', u'notes', u'in', u'the', u'audubon', u'magazine', u'and', u'later', u'as', u'books', u'in', u'birding', u'on', u'bronco', u'she', u'writes', u'an', u'engaging', u'memoir', u'about', u'her', u'several', u'trips', u'to', u'study', u'birds', u'on', u'ranch', u'in', u'california', u'in', u'the', u'late']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        \n",
    "        try:\n",
    "            a = gensim.utils.simple_preprocess(str(sentence), deacc=True)\n",
    "            \n",
    "            yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "        except:\n",
    "            \n",
    "            continue\n",
    "        \n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:2])\n",
    "del data_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaimemillan/miniconda2/lib/python2.7/site-packages/gensim/models/phrases.py:494: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'florence', u'augusta', u'merriam', u'bailey', u'was', u'an', u'american', u'ornithologist', u'and', u'nature', u'writer', u'she', u'started', u'observing', u'bird', u'behavior', u'at', u'time', u'when', u'most', u'bird', u'study', u'was', u'based', u'on', u'collections', u'and', u'skins', u'by', u'she', u'began', u'to', u'write', u'articles', u'focusing', u'on', u'protecting', u'birds', u'her', u'introduction', u'of', u'birdwatching', u'field', u'guide', u'aimed', u'at', u'living', u'birds', u'is', u'considered', u'the', u'first', u'in', u'the', u'tradition', u'of', u'modern', u'bird', u'guides', u'she', u'wrote', u'the', u'first', u'of', u'these', u'at', u'the', u'age', u'of', u'initially', u'as', u'series', u'of', u'notes', u'in', u'the', u'audubon', u'magazine', u'and', u'later', u'as', u'books', u'in', u'birding', u'on', u'bronco', u'she', u'writes', u'an', u'engaging', u'memoir', u'about', u'her', u'several', u'trips', u'to', u'study', u'birds', u'on', u'ranch', u'in', u'california', u'in', u'the', u'late']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'florence', u'augusta', u'merriam', u'bailey', u'american', u'ornithologist', u'nature', u'writer', u'start', u'observe', u'bird', u'behavior', u'time', u'bird', u'study', u'base', u'collection', u'skin', u'begin', u'write', u'article', u'focus', u'protect', u'bird', u'introduction', u'birdwatching', u'field', u'guide', u'aim', u'live', u'bird', u'consider', u'first', u'tradition', u'modern', u'bird', u'guide', u'write', u'first', u'age', u'initially', u'series', u'note', u'magazine', u'later', u'book', u'bird', u'bronco', u'write', u'engage', u'memoir', u'several', u'trip', u'study', u'bird', u'ranch', u'california', u'late'], [u'radio', u'show', u'play', u'bluegrass', u'old', u'time', u'gospel', u'music', u'unsigned', u'band', u'discuss', u'play', u'style'], [u'blend', u'soul', u'pop', u'funk', u'rhythm', u'blue', u'rock', u'song', u'musilosophy', u'singer', u'songwriter', u'jazz', u'pianist', u'lt_br', u'gt'], [u'great', u'podcast', u'host', u'libsyn'], [u'semi', u'weekly', u'discussion', u'plot', u'theme', u'highlight', u'hbo', u'series', u'game', u'throne', u'host', u'dave', u'chen', u'filmcast', u'tobolowsky', u'file', u'joanna', u'robinson', u'editor', u'pajiba', u'dish', u'praise', u'criticism', u'try', u'sound', u'pompous', u'book', u'good', u'question', u'comment', u'oath', u'fealty', u'email'], [u'ebenezer', u'scrooge', u'mean', u'old', u'miser', u'give', u'second', u'chance', u'right', u'haunt', u'ghost', u'christma', u'eve', u'dicken', u'classic', u'christma', u'carol', u'audio', u'book'], [u'idea', u'life'], [u'idea', u'life'], [u'center', u'strong', u'nothing', u'real', u'threaten', u'lt_br', u'nothing', u'unreal', u'exist', u'herein', u'lie', u'peace', u'strong', u'gt', u'center'], [u'quot', u'cuppa', u'miracle', u'quot', u'series', u'cuppa', u'miracle', u'back', u'send', u'idea', u'topic', u'would', u'like', u'show', u'join', u'minute', u'series', u'share', u'personal', u'experience', u'wisdom', u'course', u'miracl', u'lt_br', u'gt', u'show', u'create', u'moment', u'simply', u'ask', u'holy', u'spirit', u'topic', u'share', u'cup', u'coffee', u'set', u'camera', u'roll', u'lt_br', u'grab', u'cuppa', u'coffee', u'tea', u'back', u'enjoy', u'quot', u'cuppa', u'miracle', u'quot', u'note', u'audio', u'youtube', u'series', u'name', u'miraclesone', u'foundation', u'www', u'miraclesone', u'org']]\n"
     ]
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=2, \n",
    "                                           random_state=400,\n",
    "                                           update_every=100,\n",
    "                                           chunksize=10,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TOPICS = 10\n",
    "lsi_model = gensim.models.LsiModel(corpus=corpus, num_topics=N_TOPICS, id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "#print('\\nPerplexity: ', lsi_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lsi = CoherenceModel(model=lsi_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lsi = coherence_model_lsi.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LSI Model:\")\n",
    " \n",
    "for idx in range(N_TOPICS):\n",
    "    # Print the first 10 most representative topics\n",
    "    print(\"Topic #%s:\" % idx, lsi_model.print_topic(idx, 10))\n",
    " \n",
    "print(\"=\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TOPICS = 5\n",
    "lda_model = gensim.models.LdaModel(corpus=corpus, num_topics=N_TOPICS, id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LDA Model:\")\n",
    " \n",
    "for idx in range(N_TOPICS):\n",
    "    # Print the first 10 most representative topics\n",
    "    print(\"Topic #%s:\" % idx, lda_model.print_topic(idx, 10))\n",
    " \n",
    "print(\"=\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('J_tsar_tweets.csv')\n",
    "test_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a9eea11dc96f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "text = test_data.text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-4ba346c9fb70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Remove Emails\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mnew_sentence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "# Convert to list\n",
    "\n",
    "#data = new_df.content[:50].values.tolist()\n",
    "\n",
    "\n",
    "# Remove Emails\n",
    "new_data = []\n",
    "for idx,sentence in enumerate(text):\n",
    "    new_sentence=str()\n",
    "    try:\n",
    "        \n",
    "        new_sentence =  re.sub('\\S*@\\S*\\s?', '', sentence)\n",
    "        \n",
    "    except:\n",
    "         continue;\n",
    "            \n",
    "    try:\n",
    "        \n",
    "        new_sentence =  re.sub('\\s+', ' ', new_sentence)\n",
    "        \n",
    "    except:\n",
    "         continue;   \n",
    "    \n",
    "    try:\n",
    "        \n",
    "        new_sentence =  re.sub(\"\\'\", \"\", new_sentence)\n",
    "        \n",
    "    except:\n",
    "         continue;\n",
    "\n",
    "    new_data.append(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_text = list()\n",
    "for tweet in new_data[:20]:\n",
    "    tokens = tweet.split()\n",
    "    \n",
    "    string_text += tokens\n",
    "    \n",
    "#print(string_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'id2word' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-498e9ed627e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid2word\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'id2word' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "bow = id2word.doc2bow(string_text)\n",
    " \n",
    "print(lda_model[bow])\n",
    "# [(0, 0.091615426138426506), (1, -0.0085557463300508351), (2, 0.016744863677828108), (3, 0.040508186718598529), (4, 0.014201267714185898), (5, -0.012208538275305329), (6, 0.031254053085582149), (7, 0.017529584659403553), (8, 0.056957633371540077),\n",
    "#(9, 0.025989149894888153)]\n",
    " \n",
    "#print(lda_model[bow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lsi_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-0002271bf301>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlsi_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'lsi_model' is not defined"
     ]
    }
   ],
   "source": [
    "print(lsi_model[bow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lda_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-45b87b707f69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtop_words_per_topic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtop_words_per_topic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlda_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow_topic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_words_per_topic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Topic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Word'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'P'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"top_words.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lda_model' is not defined"
     ]
    }
   ],
   "source": [
    "top_words_per_topic = []\n",
    "for t in range(lda_model.num_topics):\n",
    "    top_words_per_topic.extend([(t, ) + x for x in lda_model.show_topic(t, topn = 20)])\n",
    "\n",
    "pd.DataFrame(top_words_per_topic, columns=['Topic', 'Word', 'P']).to_csv(\"top_words.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_lda = pd.read_csv('top_words.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Word</th>\n",
       "      <th>P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>right</td>\n",
       "      <td>0.024787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>series</td>\n",
       "      <td>0.022096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>show</td>\n",
       "      <td>0.020418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>quot</td>\n",
       "      <td>0.015878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>episode</td>\n",
       "      <td>0.013829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>hard</td>\n",
       "      <td>0.012986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>christma</td>\n",
       "      <td>0.012983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>keep</td>\n",
       "      <td>0.011998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>audio</td>\n",
       "      <td>0.011799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>get</td>\n",
       "      <td>0.011732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>br</td>\n",
       "      <td>0.009586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>book</td>\n",
       "      <td>0.008316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>follow</td>\n",
       "      <td>0.007318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>world</td>\n",
       "      <td>0.007299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>history</td>\n",
       "      <td>0.007240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>podcast</td>\n",
       "      <td>0.007106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>old</td>\n",
       "      <td>0.007087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>time</td>\n",
       "      <td>0.007086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>eve</td>\n",
       "      <td>0.007086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>good</td>\n",
       "      <td>0.007085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>br</td>\n",
       "      <td>0.019376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>idea</td>\n",
       "      <td>0.013865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>battle</td>\n",
       "      <td>0.012151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>place</td>\n",
       "      <td>0.012148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>life</td>\n",
       "      <td>0.012040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>amp</td>\n",
       "      <td>0.010019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>nbsp</td>\n",
       "      <td>0.009661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>0.009017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>quot</td>\n",
       "      <td>0.008975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>cuppa</td>\n",
       "      <td>0.008925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>listen</td>\n",
       "      <td>0.007653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>71</td>\n",
       "      <td>3</td>\n",
       "      <td>play</td>\n",
       "      <td>0.007650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>72</td>\n",
       "      <td>3</td>\n",
       "      <td>fine</td>\n",
       "      <td>0.007647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>73</td>\n",
       "      <td>3</td>\n",
       "      <td>groove</td>\n",
       "      <td>0.007646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>74</td>\n",
       "      <td>3</td>\n",
       "      <td>hope</td>\n",
       "      <td>0.007646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>75</td>\n",
       "      <td>3</td>\n",
       "      <td>house</td>\n",
       "      <td>0.007646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>76</td>\n",
       "      <td>3</td>\n",
       "      <td>musical</td>\n",
       "      <td>0.007646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>77</td>\n",
       "      <td>3</td>\n",
       "      <td>stroll</td>\n",
       "      <td>0.007646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>78</td>\n",
       "      <td>3</td>\n",
       "      <td>london</td>\n",
       "      <td>0.007646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>79</td>\n",
       "      <td>3</td>\n",
       "      <td>sunday</td>\n",
       "      <td>0.007645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>80</td>\n",
       "      <td>4</td>\n",
       "      <td>br</td>\n",
       "      <td>0.038597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>81</td>\n",
       "      <td>4</td>\n",
       "      <td>gt</td>\n",
       "      <td>0.015491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>82</td>\n",
       "      <td>4</td>\n",
       "      <td>center</td>\n",
       "      <td>0.015097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>83</td>\n",
       "      <td>4</td>\n",
       "      <td>play</td>\n",
       "      <td>0.015096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>84</td>\n",
       "      <td>4</td>\n",
       "      <td>strong</td>\n",
       "      <td>0.015096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>85</td>\n",
       "      <td>4</td>\n",
       "      <td>nothing</td>\n",
       "      <td>0.015085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>86</td>\n",
       "      <td>4</td>\n",
       "      <td>back</td>\n",
       "      <td>0.013021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>87</td>\n",
       "      <td>4</td>\n",
       "      <td>show</td>\n",
       "      <td>0.012378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>88</td>\n",
       "      <td>4</td>\n",
       "      <td>miracle</td>\n",
       "      <td>0.012349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>89</td>\n",
       "      <td>4</td>\n",
       "      <td>quot</td>\n",
       "      <td>0.011436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>90</td>\n",
       "      <td>4</td>\n",
       "      <td>cuppa</td>\n",
       "      <td>0.010687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>91</td>\n",
       "      <td>4</td>\n",
       "      <td>series</td>\n",
       "      <td>0.008860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>92</td>\n",
       "      <td>4</td>\n",
       "      <td>life</td>\n",
       "      <td>0.008264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>93</td>\n",
       "      <td>4</td>\n",
       "      <td>great</td>\n",
       "      <td>0.008250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>94</td>\n",
       "      <td>4</td>\n",
       "      <td>radio</td>\n",
       "      <td>0.008237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>4</td>\n",
       "      <td>herein</td>\n",
       "      <td>0.008236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>4</td>\n",
       "      <td>old</td>\n",
       "      <td>0.008235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>4</td>\n",
       "      <td>bluegrass</td>\n",
       "      <td>0.008235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>4</td>\n",
       "      <td>conquest</td>\n",
       "      <td>0.008235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>4</td>\n",
       "      <td>lie</td>\n",
       "      <td>0.008235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  Topic       Word         P\n",
       "0            0      0      right  0.024787\n",
       "1            1      0     series  0.022096\n",
       "2            2      0       show  0.020418\n",
       "3            3      0       quot  0.015878\n",
       "4            4      0    episode  0.013829\n",
       "5            5      0       hard  0.012986\n",
       "6            6      0   christma  0.012983\n",
       "7            7      0       keep  0.011998\n",
       "8            8      0      audio  0.011799\n",
       "9            9      0        get  0.011732\n",
       "10          10      0         br  0.009586\n",
       "11          11      0       book  0.008316\n",
       "12          12      0     follow  0.007318\n",
       "13          13      0      world  0.007299\n",
       "14          14      0    history  0.007240\n",
       "15          15      0    podcast  0.007106\n",
       "16          16      0        old  0.007087\n",
       "17          17      0       time  0.007086\n",
       "18          18      0        eve  0.007086\n",
       "19          19      0       good  0.007085\n",
       "20          20      1         br  0.019376\n",
       "21          21      1       idea  0.013865\n",
       "22          22      1     battle  0.012151\n",
       "23          23      1      place  0.012148\n",
       "24          24      1       life  0.012040\n",
       "25          25      1        amp  0.010019\n",
       "26          26      1       nbsp  0.009661\n",
       "27          27      1     series  0.009017\n",
       "28          28      1       quot  0.008975\n",
       "29          29      1      cuppa  0.008925\n",
       "..         ...    ...        ...       ...\n",
       "70          70      3     listen  0.007653\n",
       "71          71      3       play  0.007650\n",
       "72          72      3       fine  0.007647\n",
       "73          73      3     groove  0.007646\n",
       "74          74      3       hope  0.007646\n",
       "75          75      3      house  0.007646\n",
       "76          76      3    musical  0.007646\n",
       "77          77      3     stroll  0.007646\n",
       "78          78      3     london  0.007646\n",
       "79          79      3     sunday  0.007645\n",
       "80          80      4         br  0.038597\n",
       "81          81      4         gt  0.015491\n",
       "82          82      4     center  0.015097\n",
       "83          83      4       play  0.015096\n",
       "84          84      4     strong  0.015096\n",
       "85          85      4    nothing  0.015085\n",
       "86          86      4       back  0.013021\n",
       "87          87      4       show  0.012378\n",
       "88          88      4    miracle  0.012349\n",
       "89          89      4       quot  0.011436\n",
       "90          90      4      cuppa  0.010687\n",
       "91          91      4     series  0.008860\n",
       "92          92      4       life  0.008264\n",
       "93          93      4      great  0.008250\n",
       "94          94      4      radio  0.008237\n",
       "95          95      4     herein  0.008236\n",
       "96          96      4        old  0.008235\n",
       "97          97      4  bluegrass  0.008235\n",
       "98          98      4   conquest  0.008235\n",
       "99          99      4        lie  0.008235\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_words_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
